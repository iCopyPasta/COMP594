{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.utils\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "from PIL import Image\n",
    "import DrawingWithTensors\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from torchvision.transforms import ToPILImage\n",
    "#from IPython.display import Image\n",
    "#to_img = ToPILImage()\n",
    "#from IPython.display import Image\n",
    "\n",
    "#plt.ion()   # interactive mode\n",
    "\n",
    "#original code for training: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "\n",
    "#original paths for FCNs:\n",
    "#/home/peo5032/data/models/chainer/fcn16s_from_caffe.npz\n",
    "# calling torchfcn.models.FCN16s.pretrained_model yields:\n",
    "# might need to call download on it first: torchfcn.models.FCN16s.download()\n",
    "#'/home/peo5032/data/models/pytorch/fcn16s_from_caffe.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['-d', '--dataset'], dest='dataset', nargs=None, const=None, default=None, type=None, choices=None, help='root directory of dataset folder', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "# initiate the parser\n",
    "parser = argparse.ArgumentParser(description = \"List of options to run application when creating custom datset\")\n",
    "\n",
    "parser = argparse.ArgumentParser()  \n",
    "parser.add_argument(\"-V\", \"--version\", help=\"show program version\", action=\"store_true\")\n",
    "parser.add_argument(\"-b\", \"--batch\", help=\"batch size in each epoch\")\n",
    "parser.add_argument(\"-e\", \"--epoch\", help=\"number of epochs for training\")\n",
    "parser.add_argument(\"-r\", \"--root_folder\", help=\"destination for root folder\")\n",
    "parser.add_argument(\"-i\", \"--iteration\", help=\"which generation number we are using\")\n",
    "parser.add_argument(\"-t\", \"--training\", help=\"true/false to start with new Unet weights\")\n",
    "parser.add_argument(\"-w\", \"--weights\", help=\"full path to save weights\")\n",
    "parser.add_argument(\"-c\", \"--pickup\", help=\"full path to resume training use weights\")\n",
    "parser.add_argument(\"-p\", \"--picture\", help=\"picture dimensions\")\n",
    "parser.add_argument(\"-d\", \"--dataset\", help=\"root directory of dataset folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size is set to 1\n",
      "number of epochs is set to 29900\n",
      "iteration is set to 5\n",
      "save location is set to /home/peo5032/Documents/COMP594/modelAlbu1\n",
      "new training is set to true\n",
      "picture size to train on is 416\n"
     ]
    }
   ],
   "source": [
    "PRETRAINED_PATH = '/home/peo5032/data/models/pytorch/fcn16s_from_caffe.pth'\n",
    "\n",
    "NUM_CLASSES = 1\n",
    "EPOCHS = 4\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "imageSize = 416\n",
    "batchSize = 1\n",
    "iteration = \"1\"\n",
    "newTraining = False\n",
    "\n",
    "#change values if user specifies non-default values\n",
    "#'-t','True','-i','2','-e','325','-b','1', '-w', '/home/peo5032/Documents/COMP594/model2.pt', '-p','416'\n",
    "#args = parser.parse_args(['-i','2','-e','39500','-b','1', '-w', '/home/peo5032/Documents/COMP594/model4.pt', '-p','416','-c','/home/peo5032/Documents/COMP594/bce/INTERRUPTED4.pt']\n",
    "args = parser.parse_args(['-t','true','-i','5','-e','29900',\n",
    "                          '-b','1', '-w', '/home/peo5032/Documents/COMP594/modelAlbu1', '-p','416'])\n",
    "\n",
    "# check for --version or -V\n",
    "if args.version:  \n",
    "    print(\"this is version 0.1\")\n",
    "    \n",
    "if args.batch: \n",
    "    print(\"batch size is set to\", args.batch)\n",
    "    batchSize = int(args.batch)\n",
    "\n",
    "if args.epoch: \n",
    "    print(\"number of epochs is set to\", args.epoch)\n",
    "    EPOCHS = int(args.epoch)\n",
    "    \n",
    "if args.root_folder:  \n",
    "    if os.path.exists(root_folder):\n",
    "        ROOT = root_folder\n",
    "    print(\"destination was\", args.root_folder)\n",
    "    \n",
    "if args.iteration:\n",
    "    print(\"iteration is set to\", args.iteration)\n",
    "    iteration = args.iteration\n",
    "\n",
    "    \n",
    "SAVE_LOCATION = \"/home/peo5032/Documents/COMP594/input/gen\"+iteration+\"/model.pt\"\n",
    "\n",
    "if args.weights:\n",
    "    print(\"save location is set to\", args.weights)\n",
    "    SAVE_LOCATION = args.weights\n",
    "    \n",
    "    \n",
    "LOAD_LOCATION = \"/home/peo5032/Documents/COMP594/input/gen\"+iteration+\"/model.pt\"\n",
    "\n",
    "\n",
    "if args.pickup:\n",
    "    print(\"load location is set to\", args.pickup)\n",
    "    LOAD_LOCATION = args.pickup\n",
    "\n",
    "    \n",
    "data_dir = '/home/peo5032/Documents/COMP594/input/gen'+iteration\n",
    "\n",
    "if args.dataset:\n",
    "    print(\"loading dataset from \", args.dataset)\n",
    "    data_dir = args.dataset\n",
    "    \n",
    "if args.training:\n",
    "    if args.training.lower() == \"true\":\n",
    "        print(\"new training is set to true\")\n",
    "        newTraining = True\n",
    "        \n",
    "if args.picture:\n",
    "    print(\"picture size to train on is\", args.picture)\n",
    "    imageSize = int(args.picture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DICELossMultiClass(torch.nn.Module):\n",
    " \n",
    "    def __init__(self):\n",
    "        super(DICELossMultiClass, self).__init__()\n",
    " \n",
    "    def forward4(self, pred, targs):\n",
    "        pred = (pred>0).float()\n",
    "        return 2. * (pred*targs).sum() / (pred+targs).sum()\n",
    "    \n",
    "    #https://gist.github.com/weiliu620/52d140b22685cf9552da4899e2160183#file-dice_coeff_loss-py-L3\n",
    "    def forward3(self, pred, target):\n",
    "        \"\"\"This definition generalize to real valued pred and target vector.\n",
    "    This should be differentiable.\n",
    "        pred: tensor with first dimension as batch\n",
    "        target: tensor with first dimension as batch\n",
    "        \"\"\"\n",
    "\n",
    "        smooth = .0000001\n",
    "\n",
    "        # have to use contiguous since they may from a torch.view op\n",
    "        iflat = pred.contiguous().view(-1)\n",
    "        tflat = target.contiguous().view(-1)\n",
    "        intersection = (iflat * tflat).sum()\n",
    "\n",
    "        A_sum = torch.sum(tflat * iflat)\n",
    "        B_sum = torch.sum(tflat * tflat)\n",
    "\n",
    "        return 1 - ((2. * intersection + smooth) / (A_sum + B_sum + smooth) )\n",
    "    \n",
    "    \n",
    "    #self implementation\n",
    "    def forward2(self, input, target):\n",
    "        #self.save_for_backward(input, target)\n",
    "        eps = 0.0001\n",
    "        #self.inter = torch.dot(input.view(-1), target.view(-1))\n",
    "        self.inter = 2. * torch.dot(input.abs().view(-1), target.view(-1))\n",
    "        self.union = torch.sum(torch.mul(input,input).abs()) + torch.sum(target) \n",
    "\n",
    "        t = self.inter / self.union.float()\n",
    "        return 1-t\n",
    "    \n",
    "    # From: https://github.com/pytorch/pytorch/issues/1249\n",
    "    def forward(self,input, target):\n",
    "        input = torch.sigmoid(input)\n",
    "\n",
    "        iflat = input.view(-1)\n",
    "        tflat = target.view(-1)\n",
    "        intersection = (iflat * tflat).sum()\n",
    "\n",
    "        return 1 - ((2. * intersection) /\n",
    "                  (iflat.sum() + tflat.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([transforms.Resize([imageSize,imageSize]),\n",
    "                                      transforms.ToTensor()\n",
    "                                     ])\n",
    "\n",
    "# instantiate the dataset and dataloader\n",
    "dataset = ImageFolderWithPaths(data_dir, transform=data_transforms) # our custom dataset\n",
    "dataloaders = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle=False)\n",
    "\n",
    "# iterate over data\n",
    "#for inputs, labels, paths in dataloader:\n",
    "#    # use the above variables freely\n",
    "#    print(inputs, labels, paths)\n",
    "\n",
    "#groundTruth = tensor\n",
    "#label = tensor[0,0]\n",
    "#path = tuple list, access each via path[index]\n",
    "\n",
    "new_road_factory = DrawingWithTensors.datasetFactory(IMAGE_SIZE=416)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Routine without Validation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=4):\n",
    "    since = time.time()\n",
    "    best_model = None\n",
    "    best_loss = math.inf\n",
    "    model.train()  # Set model to training mode\n",
    "    for epoch in range(1,num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1), flush=True)\n",
    "        print('-' * 10, flush=True)\n",
    "\n",
    "        epoch_loss = 0\n",
    "   \n",
    "        #BATCH TUPLE\n",
    "        inputs, labels, paths = next(iter(dataloaders))\n",
    "        inputs.to(device)\n",
    "        #print(inputs.size())\n",
    "                \n",
    "        #build ground-truth batch tensor\n",
    "        for locations in paths:\n",
    "            i = 0\n",
    "            #dtype=torch.int64\n",
    "            labels = torch.zeros(batchSize,NUM_CLASSES,imageSize,imageSize, dtype = torch.float32).to(device)\n",
    "            labels[i] = torch.load(locations.replace(\".png\", \".pt\").replace(\"roads\", \"tensor_values\")) #manually fetch your own tensor values here somehow? \n",
    "            i += 1\n",
    "            \n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        # TODO: ENSURE OUTPUTS AND GROUNDTRUTH ARE THE SAME\n",
    "        with torch.set_grad_enabled(True):\n",
    "            #build input-truth batch tensor\n",
    "            outputs = model(inputs.to(device)).to(device)\n",
    "            loss = criterion(outputs, labels) #ground truth comparison\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # backward + optimize \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # statistics\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        print('Epoch finished', flush=True)\n",
    "        #print(\"BCE: {}\".format(epoch_loss), flush=True)\n",
    "        print(\"dice: {}\".format(epoch_loss), flush=True)\n",
    "        #print('loss: {}'.format(epoch_loss), flush=True)\n",
    "\n",
    "        #save best copy of  model\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(model, SAVE_LOCATION.replace(\"model\", \"model_best\"))\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    \n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60), flush=True)\n",
    "\n",
    "    #completed model\n",
    "    torch.save(model,SAVE_LOCATION)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showInferenceOnImage(img, tensor, class_label, threshold, classMap):\n",
    "    IMAGE_SIZE = 416\n",
    "    imgTMP = img.copy()\n",
    "    imgMap = imgTMP.load()\n",
    "    class_type_corresponding_channel = classMap[class_label]\n",
    "    print(\"index for channel\", class_label, \":\", class_type_corresponding_channel)    \n",
    "    for i in range(0, IMAGE_SIZE):\n",
    "        for j in range(0, IMAGE_SIZE):\n",
    "            if tensor[class_type_corresponding_channel, i,j] < threshold:\n",
    "                #hide background and emphasize the places where the network thinks are \"class type\"\n",
    "                imgMap[i,j] = (0,0,0)\n",
    "        \n",
    "    return imgTMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports related to UNet\n",
    "from unet_models import *\n",
    "\n",
    "if newTraining:\n",
    "    model = AlbuNet(num_classes=1, num_filters=32, pretrained=True, is_deconv=True)\n",
    "    \n",
    "else:\n",
    "    print(\"loading weights from\", LOAD_LOCATION)\n",
    "    model = torch.load(LOAD_LOCATION)\n",
    "    \n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#item1 = torch.ones(1,30,2,2)\n",
    "#item2 = torch.ones(1,30,2,2)\n",
    "\n",
    "#crit = DICELoss()\n",
    "#crit = torch.nn.BCEWithLogitsLoss()\n",
    "#loss = crit(item1, item2)\n",
    "#print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.sigmoid(item1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/29899\n",
      "----------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 1. Got 13 and 12 in dimension 2 at /opt/conda/conda-bld/pytorch_1532579805626/work/aten/src/THC/generic/THCTensorMath.cu:87",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0eb294069325>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-536097c7ed45>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m#build input-truth batch tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#ground truth comparison\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/COMP594/UNetExperiments/unet_models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mcenter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mdec5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mdec4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdec5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 1. Got 13 and 12 in dimension 2 at /opt/conda/conda-bld/pytorch_1532579805626/work/aten/src/THC/generic/THCTensorMath.cu:87"
     ]
    }
   ],
   "source": [
    "#criterion = torch.nn.BCEWithLogitsLoss()\n",
    "#criterion = torch.nn.BCELoss()\n",
    "#criterion = DiceCoeff()\n",
    "criterion = DICELossMultiClass()\n",
    "\n",
    "# Observe default choices, except using amsgrad version of Adam\n",
    "optimizer_ft = optim.Adam(model.parameters(),amsgrad=True)\n",
    "\n",
    "# Osscilate between high and low learning rates\n",
    "exp_lr_scheduler = lr_scheduler.CosineAnnealingLR(optimizer_ft, 7)\n",
    "\n",
    "try:\n",
    "    model = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=EPOCHS)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    torch.save(model, SAVE_LOCATION.replace(\"model\",'INTERRUPTED'))\n",
    "    print('Saved interrupt', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show results in meanwhile\n",
    "img = Image.open(\"/home/peo5032/Documents/COMP594/input/gen5/roads/29995.png\")\n",
    "test_tensor = torch.load(\"/home/peo5032/Documents/COMP594/input/gen5/tensor_values/29995.pt\").to(device)\n",
    "inputs = torch.zeros(1,3, imageSize, imageSize).to(device)\n",
    "\n",
    "test_tensor.size()\n",
    "inputs[0] = transforms.ToTensor()(img)\n",
    "outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0][0][410]\n",
    "#torch.sigmoid(outputs[0][0][410])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_road_factory.classMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"min\", torch.min(outputs[0]), \"max\", torch.max(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"min\", torch.min(torch.sigmoid(outputs[0])), \"max\", torch.max(torch.sigmoid(outputs[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label = \"road\"\n",
    "classMap = new_road_factory.classMap\n",
    "threshold = 0.6\n",
    "showInferenceOnImage(img, torch.sigmoid(outputs[0]), class_label, threshold, classMap)\n",
    "#showInferenceOnImage(img, test_tensor, class_label, threshold, classMap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
