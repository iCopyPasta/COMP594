{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.utils\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "from PIL import Image\n",
    "import DrawingWithTensors\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from torchvision.transforms import ToPILImage\n",
    "#from IPython.display import Image\n",
    "#to_img = ToPILImage()\n",
    "#from IPython.display import Image\n",
    "\n",
    "#plt.ion()   # interactive mode\n",
    "\n",
    "#original code for training: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "\n",
    "#original paths for FCNs:\n",
    "#/home/peo5032/data/models/chainer/fcn16s_from_caffe.npz\n",
    "# calling torchfcn.models.FCN16s.pretrained_model yields:\n",
    "# might need to call download on it first: torchfcn.models.FCN16s.download()\n",
    "#'/home/peo5032/data/models/pytorch/fcn16s_from_caffe.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "# initiate the parser\n",
    "parser = argparse.ArgumentParser(description = \"List of options to run application when creating custom datset\")\n",
    "\n",
    "parser = argparse.ArgumentParser()  \n",
    "parser.add_argument(\"-V\", \"--version\", help=\"show program version\", action=\"store_true\")\n",
    "parser.add_argument(\"-b\", \"--batch\", help=\"batch size in each epoch\")\n",
    "parser.add_argument(\"-e\", \"--epoch\", help=\"number of epochs for training\")\n",
    "parser.add_argument(\"-r\", \"--root_folder\", help=\"destination for root folder\")\n",
    "parser.add_argument(\"-i\", \"--iteration\", help=\"which generation number we are using\")\n",
    "parser.add_argument(\"-t\", \"--training\", help=\"true/false to start with new Unet weights\")\n",
    "parser.add_argument(\"-w\", \"--weights\", help=\"full path to save weights\")\n",
    "parser.add_argument(\"-c\", \"--pickup\", help=\"full path to resume training use weights\")\n",
    "parser.add_argument(\"-p\", \"--picture\", help=\"picture dimensions\")\n",
    "parser.add_argument(\"-d\", \"--dataset\", help=\"root directory of dataset folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_PATH = '/home/peo5032/data/models/pytorch/fcn16s_from_caffe.pth'\n",
    "\n",
    "NUM_CLASSES = 7\n",
    "EPOCHS = 4\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\" #just for testing for sunlab\n",
    "imageSize = 416\n",
    "batchSize = 1\n",
    "iteration = \"1\"\n",
    "newTraining = False\n",
    "\n",
    "#change values if user specifies non-default values\n",
    "#'-t','True','-i','2','-e','325','-b','1', '-w', '/home/peo5032/Documents/COMP594/model2.pt', '-p','416'\n",
    "args = parser.parse_args()\n",
    "\n",
    "# check for --version or -V\n",
    "if args.version:  \n",
    "    print(\"this is version 0.1\")\n",
    "    \n",
    "if args.batch: \n",
    "    print(\"batch size is set to\", args.batch)\n",
    "    batchSize = int(args.batch)\n",
    "\n",
    "if args.epoch: \n",
    "    print(\"number of epochs is set to\", args.epoch)\n",
    "    EPOCHS = int(args.epoch)\n",
    "    \n",
    "if args.root_folder:  \n",
    "    if os.path.exists(root_folder):\n",
    "        ROOT = root_folder\n",
    "    print(\"destination was\", args.root_folder)\n",
    "    \n",
    "if args.iteration:\n",
    "    print(\"iteration is set to\", args.iteration)\n",
    "    iteration = args.iteration\n",
    "\n",
    "    \n",
    "SAVE_LOCATION = \"/home/peo5032/Documents/COMP594/input/gen\"+iteration+\"/model.pt\"\n",
    "\n",
    "if args.weights:\n",
    "    print(\"save location is set to\", args.weights)\n",
    "    SAVE_LOCATION = args.weights\n",
    "    \n",
    "    \n",
    "LOAD_LOCATION = \"/home/peo5032/Documents/COMP594/input/gen\"+iteration+\"/model.pt\"\n",
    "\n",
    "\n",
    "if args.pickup:\n",
    "    print(\"load location is set to\", args.pickup)\n",
    "    LOAD_LOCATION = args.pickup\n",
    "\n",
    "    \n",
    "data_dir = '/home/peo5032/Documents/COMP594/input/gen'+iteration\n",
    "\n",
    "if args.dataset:\n",
    "    print(\"loading dataset from \", args.dataset)\n",
    "    data_dir = args.dataset\n",
    "    \n",
    "if args.training:\n",
    "    if args.training.lower() == \"true\":\n",
    "        print(\"new training is set to true\")\n",
    "        newTraining = True\n",
    "        \n",
    "if args.picture:\n",
    "    print(\"picture size to train on is\", args.picture)\n",
    "    imageSize = int(args.picture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DICELossMultiClass(torch.nn.Module):\n",
    " \n",
    "    def __init__(self):\n",
    "        super(DICELossMultiClass, self).__init__()\n",
    " \n",
    "    def forward4(self, pred, targs):\n",
    "        pred = (pred>0).float()\n",
    "        return 2. * (pred*targs).sum() / (pred+targs).sum()\n",
    "    \n",
    "    #https://gist.github.com/weiliu620/52d140b22685cf9552da4899e2160183#file-dice_coeff_loss-py-L3\n",
    "    def forward3(self, pred, target):\n",
    "        \"\"\"This definition generalize to real valued pred and target vector.\n",
    "    This should be differentiable.\n",
    "        pred: tensor with first dimension as batch\n",
    "        target: tensor with first dimension as batch\n",
    "        \"\"\"\n",
    "\n",
    "        smooth = .0000001\n",
    "\n",
    "        # have to use contiguous since they may from a torch.view op\n",
    "        iflat = pred.contiguous().view(-1)\n",
    "        tflat = target.contiguous().view(-1)\n",
    "        intersection = (iflat * tflat).sum()\n",
    "\n",
    "        A_sum = torch.sum(tflat * iflat)\n",
    "        B_sum = torch.sum(tflat * tflat)\n",
    "\n",
    "        return 1 - ((2. * intersection + smooth) / (A_sum + B_sum + smooth) )\n",
    "    \n",
    "    \n",
    "    #https://github.com/pytorch/pytorch/issues/1249\n",
    "    def forward2(self,input, target):\n",
    "        smooth = 1.\n",
    "\n",
    "        iflat = input.view(-1)\n",
    "        tflat = target.view(-1)\n",
    "        intersection = (iflat * tflat).sum()\n",
    "\n",
    "        return ((2. * intersection + smooth) /  (iflat.sum() + tflat.sum() + smooth))\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        #self.save_for_backward(input, target)\n",
    "        eps = 0.0001\n",
    "        #self.inter = torch.dot(input.view(-1), target.view(-1))\n",
    "        self.inter = 2. * torch.dot(input.abs().view(-1), target.view(-1))\n",
    "        self.union = torch.sum(torch.mul(input,input)) + torch.sum(target) \n",
    "\n",
    "        t = self.inter / self.union.float()\n",
    "        return 1-t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([transforms.Resize([imageSize,imageSize]),\n",
    "                                      transforms.ToTensor()\n",
    "                                     ])\n",
    "\n",
    "# instantiate the dataset and dataloader\n",
    "dataset = ImageFolderWithPaths(data_dir, transform=data_transforms) # our custom dataset\n",
    "dataloaders = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle=True, num_workers=1)\n",
    "\n",
    "# iterate over data\n",
    "#for inputs, labels, paths in dataloader:\n",
    "#    # use the above variables freely\n",
    "#    print(inputs, labels, paths)\n",
    "\n",
    "#groundTruth = tensor\n",
    "#label = tensor[0,0]\n",
    "#path = tuple list, access each via path[index]\n",
    "\n",
    "new_road_factory = DrawingWithTensors.datasetFactory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Routine without Validation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=4):\n",
    "    since = time.time()\n",
    "    best_model = None\n",
    "    best_loss = math.inf\n",
    "\n",
    "    for epoch in range(1,num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1), flush=True)\n",
    "        print('-' * 10, flush=True)\n",
    "        \n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        epoch_loss = 0\n",
    "   \n",
    "        #BATCH TUPLE\n",
    "        inputs, labels, paths = next(iter(dataloaders))\n",
    "        inputs.to(device)\n",
    "        #print(inputs.size())\n",
    "                \n",
    "        #build ground-truth batch tensor\n",
    "        for locations in paths:\n",
    "            i = 0\n",
    "            #dtype=torch.int64\n",
    "            labels = torch.zeros(batchSize,NUM_CLASSES,imageSize,imageSize, dtype = torch.float32)\n",
    "            labels[i] = torch.load(locations.replace(\".png\", \".pt\").replace(\"roads\", \"tensor_values\")) #manually fetch your own tensor values here somehow? \n",
    "            i += 1\n",
    "            \n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        # TODO: ENSURE OUTPUTS AND GROUNDTRUTH ARE THE SAME\n",
    "        with torch.set_grad_enabled(True):\n",
    "            #build input-truth batch tensor\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels) #ground truth comparison\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # backward + optimize \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # statistics\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        print('Epoch finished', flush=True)\n",
    "        print(\"dice: {}\".format(1-epoch_loss), flush=True)\n",
    "        print('loss: {}'.format(epoch_loss), flush=True)\n",
    "\n",
    "        #save best copy of  model\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(model, SAVE_LOCATION.replace(\"model\", \"model_best\"))\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}d {:.0f}h {:.0f}m {:.0f}s'.format(time_elapsed //(3600*3600), time_elapsed //3600,\n",
    "                                                                time_elapsed // 60, time_elapsed % 60), flush=True)\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showInferenceOnImage(img, tensor, class_label, threshold, classMap):\n",
    "    IMAGE_SIZE = 416\n",
    "    imgTMP = img.copy()\n",
    "    imgMap = imgTMP.load()\n",
    "    class_type_corresponding_channel = classMap[class_label]\n",
    "    print(\"index for channel\", class_label, \":\", class_type_corresponding_channel)    \n",
    "    for i in range(0, IMAGE_SIZE):\n",
    "        for j in range(0, IMAGE_SIZE):\n",
    "            if tensor[class_type_corresponding_channel, i,j] > threshold:\n",
    "                #show class label in white\n",
    "                imgMap[i,j] = (0,0,0)\n",
    "        \n",
    "    return imgTMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports related to UNet\n",
    "from unet_models import *\n",
    "\n",
    "if newTraining:\n",
    "    model = UNet16(num_classes=7, num_filters=32, pretrained=True, is_deconv=True)\n",
    "    \n",
    "else:\n",
    "    model = torch.load(LOAD_LOCATION)\n",
    "    \n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#item1 = torch.ones(1,2,2,2)\n",
    "#item2 = torch.ones(1,2,2,2)\n",
    "\n",
    "#crit = DICELossMultiClass()\n",
    "#crit = torch.nn.BCELoss()\n",
    "#loss = crit(item1, item2)\n",
    "#print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = torch.nn.BCEWithLogitsLoss()\n",
    "#criterion = torch.nn.BCELoss()\n",
    "#criterion = DiceCoeff()\n",
    "criterion = DICELossMultiClass()\n",
    "\n",
    "# Observe default choices, except using amsgrad version of Adam\n",
    "optimizer_ft = optim.Adam(model.parameters(),amsgrad=True)\n",
    "\n",
    "# Osscilate between high and low learning rates\n",
    "exp_lr_scheduler = lr_scheduler.CosineAnnealingLR(optimizer_ft, 7)\n",
    "\n",
    "try:\n",
    "    model = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=EPOCHS)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    torch.save(model, SAVE_LOCATION.replace(\"model\",'INTERRUPTED'))\n",
    "    print('Saved interrupt', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show results in meanwhile\n",
    "#img = Image.open(\"/home/peo5032/Documents/COMP594/input/gen2/roads/100.png\")\n",
    "#test_tensor = torch.load(\"/home/peo5032/Documents/COMP594/input/gen2/tensor_values/100.pt\")\n",
    "#inputs = torch.zeros(1,3, imageSize, imageSize)\n",
    "\n",
    "\n",
    "#inputs[0] = transforms.ToTensor()(img)\n",
    "#outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_road_factory.classMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_label = \"lane\"\n",
    "#classMap = new_road_factory.classMap\n",
    "#threshold = 0\n",
    "#showInferenceOnImage(img, outputs[0], class_label, threshold, classMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"min\", torch.min(outputs[0][0]), \"max\", torch.max(outputs[0][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
