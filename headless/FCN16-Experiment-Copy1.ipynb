{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.utils\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "from PIL import Image\n",
    "import DrawingWithTensors\n",
    "\n",
    "from torchvision.transforms import ToPILImage\n",
    "#from IPython.display import Image\n",
    "#to_img = ToPILImage()\n",
    "#from IPython.display import Image\n",
    "\n",
    "#plt.ion()   # interactive mode\n",
    "\n",
    "#original code for training: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "\n",
    "#imports related to fully convolutional network\n",
    "import torchfcn\n",
    "\n",
    "#original paths for FCNs:\n",
    "#/home/peo5032/data/models/chainer/fcn16s_from_caffe.npz\n",
    "# calling torchfcn.models.FCN16s.pretrained_model yields:\n",
    "# might need to call download on it first: torchfcn.models.FCN16s.download()\n",
    "#'/home/peo5032/data/models/pytorch/fcn16s_from_caffe.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "# initiate the parser\n",
    "parser = argparse.ArgumentParser(description = \"List of options to run application when creating custom datset\")\n",
    "\n",
    "parser = argparse.ArgumentParser()  \n",
    "parser.add_argument(\"-V\", \"--version\", help=\"show program version\", action=\"store_true\")\n",
    "parser.add_argument(\"-b\", \"--batch\", help=\"batch size in each epoch\")\n",
    "parser.add_argument(\"-e\", \"--epoch\", help=\"number of epochs for training\")\n",
    "parser.add_argument(\"-r\", \"--root_folder\", help=\"destination for root folder\")\n",
    "parser.add_argument(\"-i\", \"--iteration\", help=\"which generation number we are using\")\n",
    "parser.add_argument(\"-t\", \"--training\", help=\"load FCN weights on start\")\n",
    "parser.add_argument(\"-w\", \"--weights\", help=\"location to save weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_PATH = '/home/peo5032/data/models/pytorch/fcn16s_from_caffe.pth'\n",
    "SAVE_LOCATION = \"/home/peo5032/Documents/COMP594/\"\n",
    "NUM_CLASSES = 7\n",
    "EPOCHS = 4\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\" #just for testing for sunlab\n",
    "imageSize = 400\n",
    "batchSize = 1\n",
    "iteration = \"1\"\n",
    "newTraining = False\n",
    "\n",
    "#change values if user specifies non-default values\n",
    "args = parser.parse_args()\n",
    "\n",
    "# check for --version or -V\n",
    "if args.version:  \n",
    "    print(\"this is version 0.1\")\n",
    "    \n",
    "if args.batch: \n",
    "    print(\"batch size is set to\", args.batch)\n",
    "    batchSize = int(args.batch)\n",
    "\n",
    "if args.epoch: \n",
    "    print(\"number of epochs is set to\", args.epoch)\n",
    "    EPOCHS = int(args.epoch)\n",
    "    \n",
    "if args.root_folder:  \n",
    "    if os.path.exists(root_folder):\n",
    "        ROOT = root_folder\n",
    "    print(\"destination was\", args.root_folder)\n",
    "    \n",
    "if args.iteration:\n",
    "    print(\"iteration is set to\", args.iteration)\n",
    "    iteration = args.iteration\n",
    "    \n",
    "if args.training:\n",
    "    if args.training.lower() == \"true\":\n",
    "        print(\"training is set to true\")\n",
    "        newTraining = True\n",
    "        \n",
    "if args.weights:\n",
    "    print(\"save location is set to\", args.weights)\n",
    "    SAVE_LOCATION = args.weights\n",
    "\n",
    "#TODO in arguments\n",
    "# root folder location\n",
    "# saved weights location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/GautamSridhar/FCN-implementation-on-Pytorch/blob/master/DiceLoss.py\n",
    "# deleted\n",
    "\n",
    "#https://github.com/iCopyPasta/Pytorch-UNet/blob/master/dice_loss.py\n",
    "from torch.autograd import Function, Variable\n",
    "\n",
    "class DiceCoeff(Function):\n",
    "    \"\"\"Dice coeff for individual examples\"\"\"\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        self.save_for_backward(input, target)\n",
    "        eps = 0.0001\n",
    "        self.inter = torch.dot(input.view(-1), target.view(-1))\n",
    "        self.union = torch.sum(input) + torch.sum(target) + eps\n",
    "\n",
    "        t = (2 * self.inter.float() + eps) / self.union.float()\n",
    "        return t\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    def backward(self, grad_output):\n",
    "\n",
    "        input, target = self.saved_tensors\n",
    "        grad_input = grad_target = None\n",
    "\n",
    "        if self.needs_input_grad[0]:\n",
    "            grad_input = grad_output * 2 * (target * self.union + self.inter) \\\n",
    "                         / self.union * self.union\n",
    "        if self.needs_input_grad[1]:\n",
    "            grad_target = None\n",
    "\n",
    "        return grad_input, grad_target\n",
    "\n",
    "\n",
    "    def dice_coeff(input, target):\n",
    "        \"\"\"Dice coeff for batches\"\"\"\n",
    "        if input.is_cuda:\n",
    "            s = torch.FloatTensor(1).cuda().zero_()\n",
    "        else:\n",
    "            s = torch.FloatTensor(1).zero_()\n",
    "\n",
    "        for i, c in enumerate(zip(input, target)):\n",
    "            s = s + DiceCoeff().forward(c[0], c[1])\n",
    "\n",
    "        return s / (i + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([transforms.Resize([imageSize,imageSize]),\n",
    "                                      transforms.ToTensor()\n",
    "                                     ])\n",
    "\n",
    "# instantiate the dataset and dataloader\n",
    "data_dir = '/home/peo5032/Documents/COMP594/input/gen'+iteration\n",
    "dataset = ImageFolderWithPaths(data_dir, transform=data_transforms) # our custom dataset\n",
    "dataloaders = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle=True, num_workers=1)\n",
    "\n",
    "\n",
    "# iterate over data\n",
    "#for inputs, labels, paths in dataloader:\n",
    "#    # use the above variables freely\n",
    "#    print(inputs, labels, paths)\n",
    "\n",
    "#groundTruth = tensor\n",
    "#label = tensor[0,0]\n",
    "#path = tuple list, access each via path[index]\n",
    "\n",
    "new_road_factory = DrawingWithTensors.datasetFactory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Routine without Validation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=4):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = model.state_dict().copy()\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "               \n",
    "        #BATCH TUPLE\n",
    "        inputs, labels, paths = next(iter(dataloaders))\n",
    "        inputs.to(device)\n",
    "                \n",
    "        #build ground-truth batch tensor\n",
    "        for locations in paths:\n",
    "            i = 0\n",
    "            #dtype=torch.int64\n",
    "            labels = torch.zeros(batchSize,NUM_CLASSES,imageSize,imageSize, dtype = torch.float32)\n",
    "            labels[i] = torch.load(locations.replace(\".png\", \".pt\").replace(\"roads\", \"tensor_values\")) #manually fetch your own tensor values here somehow? \n",
    "            i += 1\n",
    "            \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        # TODO: ENSURE OUTPUTS AND GROUNDTRUTH ARE THE SAME\n",
    "        with torch.set_grad_enabled(True):\n",
    "            #build input-truth batch tensor\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels) #ground truth comparison\n",
    "\n",
    "            # backward + optimize \n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            \n",
    "        # statistics\n",
    "        epoch_loss = loss.item() * inputs.size(0) # unsure what this part is\n",
    "        print('epoch loss:',epoch_loss)\n",
    "        \n",
    "        \n",
    "        #running_corrects += torch.sum(preds == labels.data) # unsure what this part is\n",
    "\n",
    "        #epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        #epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "        #epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        #epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "        #print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "        #phase, running_loss, ))\n",
    "\n",
    "        # deep copy the model\n",
    "        #if phase == 'val' and epoch_acc > best_acc:\n",
    "        #    best_acc = epoch_acc\n",
    "        #    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    #print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    #model.load_state_dict(best_model_wts)\n",
    "    torch.save(model, SAVE_LOCATION + \"/model.pt\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showInferenceOnImage(img, tensor, class_label, threshold, classMap):\n",
    "    IMAGE_SIZE = 400\n",
    "    imgTMP = img.copy()\n",
    "    imgMap = imgTMP.load()\n",
    "    class_type_corresponding_channel = classMap[class_label]\n",
    "    print(\"index for channel\", class_label, \":\", class_type_corresponding_channel)    \n",
    "    for i in range(0, IMAGE_SIZE):\n",
    "        for j in range(0, IMAGE_SIZE):\n",
    "            if tensor[class_type_corresponding_channel, i,j] >= threshold:\n",
    "                #show class label in white\n",
    "                imgMap[i,j] = (0,0,0)\n",
    "        \n",
    "    return imgTMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if newTraining is True:\n",
    "    model = torchfcn.models.FCN16s()\n",
    "    model.load_state_dict(torch.load(PRETRAINED_PATH))\n",
    "    \n",
    "else:\n",
    "    model = torch.load('/home/peo5032/Documents/COMP594/model.pt')\n",
    "    \n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Architecture for New Classes and New Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if newTraining is True:  \n",
    "    model.score_fr = torch.nn.Conv2d(4096, NUM_CLASSES , kernel_size=(1, 1), stride=(1, 1))\n",
    "    torch.nn.init.uniform_(model.score_fr.weight, a=0, b=0.05)\n",
    "    torch.nn.init.uniform_(model.score_fr.bias, a=0, b=0.05)\n",
    "    #model.score_fr.weight.data.fill_(0.10)\n",
    "    #model.score_fr.bias.data.fill_(0.00)\n",
    "\n",
    "    model.score_pool4 = torch.nn.Conv2d(512, NUM_CLASSES, kernel_size=(1, 1), stride=(1, 1))\n",
    "    torch.nn.init.uniform_(model.score_pool4.weight, a=0, b=0.05)\n",
    "    torch.nn.init.uniform_(model.score_pool4.bias, a=0, b=0.05)\n",
    "    #model.score_pool4.weight.data.fill_(0.10)\n",
    "    #model.score_pool4.bias.data.fill_(0.00)\n",
    "\n",
    "    model.upscore2 = torch.nn.ConvTranspose2d(NUM_CLASSES, NUM_CLASSES, kernel_size=(4, 4), stride=(2, 2), bias=False)\n",
    "    torch.nn.init.uniform_(model.upscore2.weight, a=0, b=0.05)\n",
    "    #model.upscore2.weight.data.fill_(0.10)\n",
    "\n",
    "    model.upscore16 = torch.nn.ConvTranspose2d(NUM_CLASSES, NUM_CLASSES, kernel_size=(32, 32), stride=(16, 16), bias=False)\n",
    "    torch.nn.init.uniform_(model.upscore16.weight, a=0, b=0.05)\n",
    "    #model.upscore16.weight.data.fill_(0.10)\n",
    "    \n",
    "    torch.save(model, SAVE_LOCATION + \"/model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = torch.nn.MSELoss()\n",
    "#criterion = torch.nn.L1Loss()\n",
    "#criterion = torch.nn.BCEWithLogitsLoss()\n",
    "#criterion = torch.nn.NLLLoss()\n",
    "criterion = DiceCoeff()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model.parameters(),amsgrad=True)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 3 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
