{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.utils\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "from PIL import Image\n",
    "import DrawingWithTensors\n",
    "\n",
    "from torchvision.transforms import ToPILImage\n",
    "#from IPython.display import Image\n",
    "#to_img = ToPILImage()\n",
    "#from IPython.display import Image\n",
    "\n",
    "#plt.ion()   # interactive mode\n",
    "\n",
    "#original code for training: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "\n",
    "#imports related to fully convolutional network\n",
    "import torchfcn\n",
    "\n",
    "#original paths for FCNs:\n",
    "#/home/peo5032/data/models/chainer/fcn16s_from_caffe.npz\n",
    "# calling torchfcn.models.FCN16s.pretrained_model yields:\n",
    "# might need to call download on it first: torchfcn.models.FCN16s.download()\n",
    "#'/home/peo5032/data/models/pytorch/fcn16s_from_caffe.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['-w', '--weights'], dest='weights', nargs=None, const=None, default=None, type=None, choices=None, help='full path to save weights', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "# initiate the parser\n",
    "parser = argparse.ArgumentParser(description = \"List of options to run application when creating custom datset\")\n",
    "\n",
    "parser = argparse.ArgumentParser()  \n",
    "parser.add_argument(\"-V\", \"--version\", help=\"show program version\", action=\"store_true\")\n",
    "parser.add_argument(\"-w\", \"--weights\", help=\"full path to save weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load location is set to /home/peo5032/Documents/COMP594/model.pt\n"
     ]
    }
   ],
   "source": [
    "PRETRAINED_PATH = '/home/peo5032/data/models/pytorch/fcn16s_from_caffe.pth'\n",
    "LOAD_LOCATION = \"/home/peo5032/Documents/COMP594/model.pt\"\n",
    "\n",
    "NUM_CLASSES = 7\n",
    "EPOCHS = 4\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\" #just for testing for sunlab\n",
    "imageSize = 400\n",
    "batchSize = 1\n",
    "iteration = \"1\"\n",
    "newTraining = False\n",
    "\n",
    "#change values if user specifies non-default values\n",
    "args = parser.parse_args([\"-w\", \"/home/peo5032/Documents/COMP594/model.pt\"])\n",
    "\n",
    "# check for --version or -V\n",
    "if args.version:  \n",
    "    print(\"this is version 0.1\")\n",
    "\n",
    "    \n",
    "if args.weights:\n",
    "    print(\"load location is set to\", args.weights)\n",
    "    LOAD_LOCATION = args.weights\n",
    "    \n",
    "\n",
    "#TODO in arguments\n",
    "# root folder location\n",
    "# saved weights location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([transforms.Resize([imageSize,imageSize]),\n",
    "                                      transforms.ToTensor()\n",
    "                                     ])\n",
    "\n",
    "# instantiate the dataset and dataloader\n",
    "data_dir = '/home/peo5032/Documents/COMP594/input/gen'+iteration\n",
    "dataset = ImageFolderWithPaths(data_dir, transform=data_transforms) # our custom dataset\n",
    "dataloaders = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle=True, num_workers=1)\n",
    "\n",
    "\n",
    "new_road_factory = DrawingWithTensors.datasetFactory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Routine without Validation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showInferenceOnImage(img, tensor, class_label, threshold, classMap):\n",
    "    IMAGE_SIZE = 400\n",
    "    imgTMP = img.copy()\n",
    "    imgMap = imgTMP.load()\n",
    "    class_type_corresponding_channel = classMap[class_label]\n",
    "    print(\"index for channel\", class_label, \":\", class_type_corresponding_channel)    \n",
    "    for i in range(0, IMAGE_SIZE):\n",
    "        for j in range(0, IMAGE_SIZE):\n",
    "            if tensor[class_type_corresponding_channel, i,j] >= threshold:\n",
    "                #hide non-class label in black\n",
    "                imgMap[i,j] = (0,0,0)\n",
    "        \n",
    "    return imgTMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCN16s(\n",
       "  (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(100, 100))\n",
       "  (relu1_1): ReLU(inplace)\n",
       "  (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu1_2): ReLU(inplace)\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu2_1): ReLU(inplace)\n",
       "  (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu2_2): ReLU(inplace)\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu3_1): ReLU(inplace)\n",
       "  (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu3_2): ReLU(inplace)\n",
       "  (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu3_3): ReLU(inplace)\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu4_1): ReLU(inplace)\n",
       "  (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu4_2): ReLU(inplace)\n",
       "  (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu4_3): ReLU(inplace)\n",
       "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu5_1): ReLU(inplace)\n",
       "  (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu5_2): ReLU(inplace)\n",
       "  (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu5_3): ReLU(inplace)\n",
       "  (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (fc6): Conv2d(512, 4096, kernel_size=(7, 7), stride=(1, 1))\n",
       "  (relu6): ReLU(inplace)\n",
       "  (drop6): Dropout2d(p=0.5)\n",
       "  (fc7): Conv2d(4096, 4096, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (relu7): ReLU(inplace)\n",
       "  (drop7): Dropout2d(p=0.5)\n",
       "  (score_fr): Conv2d(4096, 7, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (score_pool4): Conv2d(512, 7, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (upscore2): ConvTranspose2d(7, 7, kernel_size=(4, 4), stride=(2, 2), bias=False)\n",
       "  (upscore16): ConvTranspose2d(7, 7, kernel_size=(32, 32), stride=(16, 16), bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(LOAD_LOCATION)\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_LOCATION_ROOT = \"/home/peo5032/Documents/COMP594/input/gen1/val\"\n",
    "#img = Image.open(TEST_LOCATION_ROOT+\"/1.png\")\n",
    "#img2 = Image.open(TEST_LOCATION_ROOT+\"/7.png\")\n",
    "#inputs = torch.zeros(2,3,400,400)\n",
    "#inputs[0] = data_transforms(img)\n",
    "#inputs[1] = data_transforms(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'background': 0,\n",
       " 'left-shoulder': 1,\n",
       " 'left-yellow-line-marker': 2,\n",
       " 'white-lane-markers': 3,\n",
       " 'lane': 4,\n",
       " 'right-white-line-marker': 5,\n",
       " 'right-shoulder': 6}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_road_factory.classMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_class = \"white-lane-markers\"\n",
    "#min_score =  torch.min(outputs[0][new_road_factory.classMap[eval_class]]).item()\n",
    "#max_score = torch.max(outputs[0][new_road_factory.classMap[eval_class]]).item()\n",
    "#middle_score = (min_score + max_score)/2\n",
    "#threshold = (middle_score + (0.10 * middle_score))\n",
    "#print(\"mind is\", min_score)\n",
    "#print(\"max is\", max_score)\n",
    "#print(\"threshold is\", threshold)\n",
    "#showInferenceOnImage(img, outputs[0], eval_class, middle_score + (0.10 * middle_score) , new_road_factory.classMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()\n",
    "criterion2 = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.zeros(1,1,4,4)\n",
    "outputs = torch.ones(1,1,4,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(inputs, outputs)\n",
    "loss2 = criterion(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.631019592285156"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.631019592285156"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
